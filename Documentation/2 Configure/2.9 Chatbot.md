# Chatbot services
- `chatbot`

# Configuration example
```json
{
    "service": "chatbot",
    "type": "hf",
    "ctx": 2048,
    "threads": -1,
    "b_threads": -2,
    "ngl": -1,
    "batch": 8,
    "ubatch": 8,
    "model": "openai-community/gpt2",
    "temp": 0.5,
    "device": "cpu",
    "multimodal": "",
    "price": 0,
    "max_length": -1
}
```

## Parameters
Some of the parameters are the same as a general service.

## New parameters
### type
The library you want to use for the chatbot.

Choose between **lcpp** for LLama-CPP-Python, **g4a** for GPT4All and **hf** for HuggingFace Transformers.
We recommend **lcpp**.

#### Parameter type
string (text)

### ctx
Context size.

The longer the context size is, the more memory and compute power you will need.

> [!NOTE]
> Only works with the `lcpp` and `g4a` library.

#### Parameter type
integer (integer number)

### threads
The amount of threads to use.

Set to *-1* to use all your CPU threads.
Set to *-2* to use the library's default.

> [!NOTE]
> Only works with the `lcpp` and `g4a` library.

#### Parameter type
integer (integer number)

### b_threads
The amount of threads to use for the batch.

Set to *-1* to use all your CPU threads.
Set to *-2* to use the same as the `threads` parameter.
Set to *-3* to use the library's default.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
integer (integer number)

### ngl
Number of layer to offload into the GPU.

Set to *-1* to offload all the layers.

> [!NOTE]
> Only works with the `lcpp` and `g4a` library.

#### Parameter type
integer (integer number)

### batch
The number of tokens to process in parallel.

More batch means faster inference speed but you will need more memory.

Also, make sure your hardware can support the batch size specified or else the inference speed will be slower.

> [!NOTE]
> Only works with the `lcpp` and `g4a` library.

#### Parameter type
integer (integer number)

### ubatch
The number of physical compute units (or CUDA cores for NVIDIA GPUs) that your GPU or CPU has.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
integer (integer number)

### model
This parameter may change it's type depending on what system you want to use.

#### hf
The model parameter is a string (text), where you write the model repository in HuggingFace.

#### g4a
The model parameter is a string (text), where you write the path to your **.gguf** model file.

#### lcpp
##### HuggingFace
If you want to auto-download the model from the HuggingFace repository, this parameter will be something like this:
```json
"model": [
    "MODEL REPOSITORY",
    "MODEL FILE NAME",
    "CHAT TEMPLATE"
]
```

**MODEL REPOSITORY** is the HuggingFace repository of the model. The repository must contain **.gguf** files.

Example: "Alcoft/gpt2-GGUF"



**MODEL FILE NAME** is the **.gguf** file name in the repository.

Example: "gpt2_Q4_K_M.gguf"



**CHAT TEMPLATE** is an optional parameter, but recommended to set. It's the name of the chat template of the model.

Example: "gpt2"

##### Local
If you want to use a local **.gguf** file from your machine, this parameter will be something like this:
```json
"model": [
    "",
    "FILE PATH",
    "CHAT TEMPLATE"
]
```

**FILE PATH** is the **.gguf** file path in your computer.

Example: "/home/myuser/gpt2_Q4_K_M.gguf"



**CHAT TEMPLATE** is an optional parameter. It's the name of the chat template of the model.

Example: "gpt2"

##### Pre-defined
If you want to use a pre-defined model, this parameter will be something like this:
```json
"model": [
    "MODEL NAME",
    "QUANT TO USE",
    ""
]
```

**MODEL NAME** is the name of the model in the pre-defined list. You can find the list in the `LibI4/Python_AI/Inference/PredefinedModels/chatbot_nf_lcpp.yaml` file.

Example: "qwen2.5-0.5b-instruct"



**QUANT TO USE** is the name of the quant you want to use for the model. You can find the available quants in the list mentioned above.

Example: "Q6_K"

#### Parameter type
string (text) or list

### temp
The temperature of the model.

More temperature means more original results, but may be inaccurate.

#### Parameter type
float (floating number)

### multimodal
Multimodal support.
It can be `video`, `audio`, `image`. Separated by spaces. You can mix multiple file types if the model supports them.

Example 1: `video image`.

Example 2: `image audio`.

Example 3: `image`.

> [!NOTE]
> Only works with the `hf` library.

#### Parameter type
string (text)

### max_length
Max allowed response length.

Set to -1 to use the server configuration.

#### Parameter type
integer (integer number)

### hf_low
Set to **true** if you have a slow CPU or not enough memory.

> [!NOTE]
> Only works with the `hf` library.

#### Parameter type
boolean (true or false)

### lcpp_use_mmap
Use mmap.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
boolean (true or false)

### lcpp_use_mlock
Use mlock.

If **true**, the system will force to keep the model in RAM.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
boolean (true or false)

### lcpp_offload_kqv
Offload K, Q and V to the GPU.

Much less VRAM consumption but slower inference.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
boolean (true or false)

### lcpp_flash_attn
Use flash attention.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
boolean (true or false)

### lcpp_rope_scaling
Rope scaling type.

Can be **linear**, **longrope**, **max_value**, **none**, **unspecified** or **yarn**.

Defaults to **unspecified**.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
string (text)

### lcpp_pooling_type
Pooling type.

Can be **unspecified**, **none**, **mean**, **cls**, **last** or **rank**.

Defaults to **unspecified**.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
string (text)

### lcpp_cache_type
Type of cache.

Can be **RAM**, **DISK** or **NONE**.

Defaults to **NONE**.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
string (text)

### lcpp_tools_in_system_prompt
Instead of using the tools parameter in the lcpp library, this will add the tools to the system prompt.

Some models might understand tools better this way.

> [!NOTE]
> Only works with the `lcpp` library.
> Also, `hf` models has this parameter always enabled, it can't be changed.

#### Parameter type
boolean (true or false)

### top_p
TopP parameter for the model.

#### Parameter type
float (floating number)

### top_k
TopK parameter for the model.

#### Parameter type
integer (integer number)

### min_p
MinP parameter for the model.

#### Parameter type
float (floating number)

### typical_p
TypicalP parameter for the model.

#### Parameter type
float (floating number)

### seed
Seed for the model.

#### Parameter type
integer (integer number)