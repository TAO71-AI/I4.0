# Chatbot services
- `chatbot`

# Configuration example
```json
{
    "service": "chatbot",
    "type": "hf",
    "ctx": 2048,
    "threads": -1,
    "b_threads": -2,
    "ngl": -1,
    "batch": 8,
    "ubatch": 8,
    "model": "openai-community/gpt2",
    "temp": 0.5,
    "device": "cpu",
    "multimodal": "",
    "price": 0,
    "max_length": -1
}
```

## Parameters
Some of the parameters are the same as a general service.

## New parameters
### type
The library you want to use for the chatbot.

Choose between **lcpp** for LLama-CPP-Python,  and **hf** for HuggingFace Transformers.
We recommend **lcpp**.

#### Parameter type
string (text)

### ctx
Context size.

The longer the context size is, the more memory and compute power you will need.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
integer (integer number)

### threads
The amount of threads to use.

Set to *-1* to use all your CPU threads.
Set to *-2* to use the library's default.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
integer (integer number)

### b_threads
The amount of threads to use for the batch.

Set to *-1* to use all your CPU threads.
Set to *-2* to use the same as the `threads` parameter.
Set to *-3* to use the library's default.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
integer (integer number)

### ngl
Number of layer to offload into the GPU.

Set to *-1* to offload all the layers.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
integer (integer number)

### batch
The number of tokens to process in parallel.

More batch means faster inference speed but you will need more memory.

Also, make sure your hardware can support the batch size specified or else the inference speed will be slower.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
integer (integer number)

### ubatch
The number of physical compute units (or CUDA cores for NVIDIA GPUs) that your GPU or CPU has.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
integer (integer number)

### model
This parameter may change it's type depending on what system you want to use.

#### hf
You write the model's repository in HuggingFace. Example: `openai-community/gpt2`.

#### lcpp
##### Non-multimodal
You write the model's GGUF file path in your system. Example: `/home/user/models/gpt2_Q8_0.gguf`.

##### Multimodal
The model parameter will be a dictionary that follows the next template:
```json
{
    "model_path": "PATH TO THE MODEL'S GGUF FILE PATH IN YOUR SYSTEM",
    "mmproj": "PATH TO THE MODEL'S MMPROJ GGUF FILE PATH IN YOUR SYSTEM",
    "handler": "CHAT HANDLER. AVAILABLE HANDLERS ARE: `llava-1.5`, `llava-1.6`, `moondream2`, `nanollava`, `llama-3-vision-alpha`, `minicpm-v-2.6`, `qwen2.5-vl`. ANY MODEL THAT DOESN'T USE ANY OF THESE HANDLERS WILL NOT LOAD."
}
```

#### Parameter type
string (text) or dictionary

### temp
The temperature of the model.

More temperature means more original results, but may be inaccurate.

#### Parameter type
float (floating number)

### multimodal
Multimodal support.
It can be `video`, `audio`, `image`. Separated by spaces. You can mix multiple file types if the model supports them.

Example 1: `video image`.

Example 2: `image audio`.

Example 3: `image`.

#### Parameter type
string (text)

### max_length
Max allowed response length.

Set to -1 or null to use the server configuration.

#### Parameter type
integer (integer number)

### hf_low
Set to **true** if you have a slow CPU or not enough memory.

> [!NOTE]
> Only works with the `hf` library.

#### Parameter type
boolean (true or false)

### lcpp_use_mmap
Use mmap.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
boolean (true or false)

### lcpp_use_mlock
Use mlock.

If **true**, the system will force to keep the model in RAM.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
boolean (true or false)

### lcpp_offload_kqv
Offload K, Q and V to the GPU.

Much less VRAM consumption but slower inference.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
boolean (true or false)

### flash_attn
Use Flash Attention.

#### Parameter type
boolean (true or false)

### lcpp_rope_scaling
Rope scaling type.

Can be **linear**, **longrope**, **max_value**, **none**, **unspecified** or **yarn**.

Defaults to **unspecified**.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
string (text)

### lcpp_pooling_type
Pooling type.

Can be **unspecified**, **none**, **mean**, **cls**, **last** or **rank**.

Defaults to **unspecified**.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
string (text)

### lcpp_cache_type
Type of cache.

Can be **RAM**, **DISK** or **NONE**.

Defaults to **NONE**.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
string (text)

### tools_in_system_prompt
Instead of using the tools parameter in the library, this will add the tools to the system prompt.

Some models might understand tools better this way.

#### Parameter type
boolean (true or false)

### top_p
TopP parameter for the model.

#### Parameter type
float (floating number)

### top_k
TopK parameter for the model.

#### Parameter type
integer (integer number)

### min_p
MinP parameter for the model.

#### Parameter type
float (floating number)

### typical_p
TypicalP parameter for the model.

#### Parameter type
float (floating number)

### seed
Seed for the model.

#### Parameter type
integer (integer number)

### main_gpu
The main GPU where the model will be loaded.

> [!NOTE]
> Only works with the `lcpp` library for now.

#### Parameter type
integer (integer number)

### fill_ctx_at_start
Fills the entire model CTX at the start of the model. This way you can know how much memory it will exactly need.

Takes more time to start the server.

#### Parameter type
boolean (true or false)

### split_mode
Split mode for the model. Can be `none` (default), `row`, or `layer`.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
string (text)

### lcpp_ftype_k
Quantization type for ftype_k. Defaults to `None`.

Please do not change this value unless you know what you're doing.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
string (text)

### lcpp_ftype_v
Quantization type for ftype_v. Defaults to `None`.

Please do not change this value unless you know what you're doing.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
string (text)

### reasoning
This parameter should look something like this:
```json
{
    "reasoning": {
        "extra_kwargs": {
            "think": {},
            "no_think": {}
        },
        "system_prompt": {
            "think": {
                "token": "",
                "position": "`start` or `end`"
            },
            "no_think": {
                "token": "",
                "position": "`start` or `end`"
            }
        },
        "user_prompt": {
            "think": {
                "token": "",
                "position": "`start` or `end`"
            },
            "no_think": {
                "token": "",
                "position": "`start` or `end`"
            }
        }
    }
}
```

**extra_kwargs** is where you set the extra kwargs to pass to the model when doing inference. For example, some hf models support the *enable_thinking* argument, where you can set it to `True` or `False`. To add an extra kwargs, add `"KWARG_NAME": KWARG VALUE` to the **think** or **no_think** dictionary inside this **extra_kwargs** parameter, wether you want it to be activated when the model will think or when the model will not think.

**system_prompt** is where you set a custom token to give to the model via system prompts to enable or disable thinking. For example, Qwen3-14B supports the `/think` and `/no_think` tokens. You write the special token in the value for **token** and write the position, wether you want it to be given before or after the system prompt.

**user_prompt** is basically the same as the **system_prompt** parameter, but instead of adding the token to the system prompt, it adds it to the user's prompt (for each prompt). If you're using a Qwen3 model with lcpp this is the best option, since lcpp doesn't support the `enable_thinking` argument.

#### Parameter type
dictionary