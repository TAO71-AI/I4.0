# Chatbot services
- `chatbot`

# Configuration example
```json
{
    "service": "chatbot",
    "type": "hf",
    "ctx": 2048,
    "threads": -1,
    "b_threads": -2,
    "ngl": -1,
    "batch": 8,
    "ubatch": 8,
    "model": "openai-community/gpt2",
    "temp": 0.5,
    "device": "cpu",
    "multimodal": "",
    "price": 0,
    "max_length": -1
}
```

## Parameters
Some of the parameters are the same as a general service.

## New parameters
### type
The library you want to use for the chatbot.

Choose between **lcpp** for LLama-CPP-Python,  and **hf** for HuggingFace Transformers.
We recommend **lcpp**.

#### Parameter type
string (text)

### ctx
Context size.

The longer the context size is, the more memory and compute power you will need.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
integer (integer number)

### threads
The amount of threads to use.

Set to *-1* to use all your CPU threads.
Set to *-2* to use the library's default.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
integer (integer number)

### b_threads
The amount of threads to use for the batch.

Set to *-1* to use all your CPU threads.
Set to *-2* to use the same as the `threads` parameter.
Set to *-3* to use the library's default.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
integer (integer number)

### ngl
Number of layer to offload into the GPU.

Set to *-1* to offload all the layers.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
integer (integer number)

### batch
The number of tokens to process in parallel.

More batch means faster inference speed but you will need more memory.

Also, make sure your hardware can support the batch size specified or else the inference speed will be slower.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
integer (integer number)

### ubatch
The number of physical compute units (or CUDA cores for NVIDIA GPUs) that your GPU or CPU has.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
integer (integer number)

### model
This parameter may change it's type depending on what system you want to use.

#### hf
You write the model's repository in HuggingFace. Example: `openai-community/gpt2`.

#### lcpp
##### Non-multimodal
You write the model's GGUF file path in your system. Example: `/home/user/models/gpt2_Q8_0.gguf`.

##### Multimodal
The model parameter will be a dictionary that follows the next template:
```json
{
    "model_path": "PATH TO THE MODEL'S GGUF FILE PATH IN YOUR SYSTEM",
    "mmproj": "PATH TO THE MODEL'S MMPROJ GGUF FILE PATH IN YOUR SYSTEM",
    "handler": "CHAT HANDLER. AVAILABLE HANDLERS ARE: `llava-1.5`, `llava-1.6`, `moondream2`, `nanollava`, `llama-3-vision-alpha`, `minicpm-v-2.6`, `qwen2.5-vl`. ANY MODEL THAT DOESN'T USE ANY OF THESE HANDLERS WILL NOT LOAD."
}

#### Parameter type
string (text) or dictionary

### temp
The temperature of the model.

More temperature means more original results, but may be inaccurate.

#### Parameter type
float (floating number)

### multimodal
Multimodal support.
It can be `video`, `audio`, `image`. Separated by spaces. You can mix multiple file types if the model supports them.

Example 1: `video image`.

Example 2: `image audio`.

Example 3: `image`.

#### Parameter type
string (text)

### max_length
Max allowed response length.

Set to -1 to use the server configuration.

#### Parameter type
integer (integer number)

### hf_low
Set to **true** if you have a slow CPU or not enough memory.

> [!NOTE]
> Only works with the `hf` library.

#### Parameter type
boolean (true or false)

### lcpp_use_mmap
Use mmap.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
boolean (true or false)

### lcpp_use_mlock
Use mlock.

If **true**, the system will force to keep the model in RAM.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
boolean (true or false)

### lcpp_offload_kqv
Offload K, Q and V to the GPU.

Much less VRAM consumption but slower inference.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
boolean (true or false)

### lcpp_flash_attn
Use flash attention.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
boolean (true or false)

### lcpp_rope_scaling
Rope scaling type.

Can be **linear**, **longrope**, **max_value**, **none**, **unspecified** or **yarn**.

Defaults to **unspecified**.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
string (text)

### lcpp_pooling_type
Pooling type.

Can be **unspecified**, **none**, **mean**, **cls**, **last** or **rank**.

Defaults to **unspecified**.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
string (text)

### lcpp_cache_type
Type of cache.

Can be **RAM**, **DISK** or **NONE**.

Defaults to **NONE**.

> [!NOTE]
> Only works with the `lcpp` library.

#### Parameter type
string (text)

### tools_in_system_prompt
Instead of using the tools parameter in the library, this will add the tools to the system prompt.

Some models might understand tools better this way.

#### Parameter type
boolean (true or false)

### top_p
TopP parameter for the model.

#### Parameter type
float (floating number)

### top_k
TopK parameter for the model.

#### Parameter type
integer (integer number)

### min_p
MinP parameter for the model.

#### Parameter type
float (floating number)

### typical_p
TypicalP parameter for the model.

#### Parameter type
float (floating number)

### seed
Seed for the model.

#### Parameter type
integer (integer number)

### main_gpu
The main GPU where the model will be loaded.

> [!NOTE]
> Only works with the `lcpp` library for now.

#### Parameter type
integer (integer number)

### fill_ctx_at_start
Fills the entire model CTX at the start of the model. This way you can know how much memory it will exactly need.

Takes more time to start the server.

#### Parameter type
boolean (true or false)