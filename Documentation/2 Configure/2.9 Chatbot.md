# Chatbot services
- `chatbot`

# Configuration example
```json
{
    "service": "chatbot",
    "type": "hf",
    "ctx": 2048,
    "threads": -1,
    "ngl": -1,
    "batch": 8,
    "model": "openai-community/gpt2",
    "temp": 0.5,
    "device": "cpu",
    "allows_files": false,
    "price": 0
}
```

## Parameters
Some of the parameters are the same as a general service.

## New parameters
### type
The library you want to use for the chatbot.

Choose between **lcpp** for LLama-CPP-Python, **g4a** for GPT4All and **hf** for HuggingFace Transformers.
We recommend **lcpp**.

#### Parameter type
string (text)

### ctx
Context size.

The longer the context size is, the more memory and compute power you will need.

> [!NOTE]
> Only works with the `lcpp` and `g4a` library.

#### Parameter type
integer (integer number)

### threads
The amount of threads to use.

Set to *-1* to use all your CPU threads.
Set to *-2* to use the library's default.

> [!NOTE]
> Only works with the `lcpp` and `g4a` library.

#### Parameter type
integer (integer number)

### ngl
Number of layer to offload into the GPU.

Set to *-1* to offload all the layers.

> [!NOTE]
> Only works with the `lcpp` and `g4a` library.

#### Parameter type
integer (integer number)

### batch
The number of tokens to process in parallel.

More batch means faster inference speed but you will need more memory.

Also, make sure your hardware can support the batch size specified or else the inference speed will be slower.

> [!NOTE]
> Only works with the `lcpp` and `g4a` library.

#### Parameter type
integer (integer number)

### model
This parameter may change it's type depending on what system you want to use.

#### hf
The model parameter is a string (text), where you write the model repository in HuggingFace.

#### g4a
The model parameter is a string (text), where you write the path to your **.gguf** model file.

#### lcpp
##### HuggingFace
If you want to auto-download the model from the HuggingFace repository, this parameter will be something like this:
```json
"model": [
    "MODEL REPOSITORY",
    "MODEL FILE NAME",
    "CHAT TEMPLATE"
]
```

**MODEL REPOSITORY** is the HuggingFace repository of the model. The repository must contain **.gguf** files.

Example: "Alcoft/gpt2-GGUF"



**MODEL FILE NAME** is the **.gguf** file name in the repository.

Example: "gpt2_Q4_K_M.gguf"



**CHAT TEMPLATE** is an optional parameter, but recommended to set. It's the name of the chat template of the model.

Example: "gpt2"

##### Local
If you want to use a local **.gguf** file from your machine, this parameter will be something like this:
```json
"model": [
    "",
    "FILE PATH",
    "CHAT TEMPLATE"
]
```

**FILE PATH** is the **.gguf** file path in your computer.

Example: "/home/myuser/gpt2_Q4_K_M.gguf"



**CHAT TEMPLATE** is an optional parameter, but recommended to set. It's the name of the chat template of the model.

Example: "gpt2"

##### Pre-defined
If you want to use a pre-defined model, this parameter will be something like this:
```json
"model": [
    "MODEL NAME",
    "QUANT TO USE",
    ""
]
```

**MODEL NAME** is the name of the model in the pre-defined list. You can find the list in the `LibI4/Python_AI/Inference/PredefinedModels/chatbot_nf_lcpp.yaml` file.

Example: "qwen2.5-0.5b-instruct"



**QUANT TO USE** name of the quant you want to use for the model. You can find the available quants in the list mentioned above.

Example: "Q6_K"

#### Parameter type
string (text) or list

### temp
The temperature of the model.

More temperature means more original results, but may be inaccurate.

#### Parameter type
float (floating number)

### allows_files
Set to **true** if the chatbot you want to use is multimodal.

> [!NOTE]
> Only works with the `hf` library.

#### Parameter type
boolean (true or false)

### hf_low
Set to **true** if you have a slow CPU or not enough memory.

> [!NOTE]
> Only works with the `hf` library.

#### Parameter type
boolean (true or false)